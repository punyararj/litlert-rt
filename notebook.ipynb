{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T08:19:20.492012Z",
     "start_time": "2025-05-24T08:19:20.272034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from litert_tools.pipeline import pipeline\n",
    "from litert_tools.pipeline import task_file_processor as task_file_processor_lib\n",
    "from litert_tools.pipeline import tokenizer as tokenizer_lib\n",
    "from litert_tools.pipeline.pipeline import LiteRTLlmPipeline\n",
    "from ai_edge_litert import interpreter as interpreter_lib\n",
    "from typing import Optional\n",
    "import sentencepiece as sp"
   ],
   "id": "e3bd9dfdc17e1ae7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtreemer/PycharmProjects/LiteRTPOC/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T08:19:20.978618Z",
     "start_time": "2025-05-24T08:19:20.974781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load(\n",
    "      filename: str,\n",
    "      tokenizer_location: Optional[str] = None,\n",
    "  ) -> LiteRTLlmPipeline:\n",
    "\n",
    "    try:\n",
    "      if filename and filename.endswith(\".task\"):\n",
    "        # Extract tflite, tokenizer and metadata from .task bundle\n",
    "        file_processor = task_file_processor_lib.TaskFileProcessor(\n",
    "            filename, cache_dir='cache'\n",
    "        )\n",
    "        model_path = file_processor.get_tflite_file_path()\n",
    "\n",
    "        tokenizer_path = file_processor.get_tokenizer_file_path()\n",
    "        raw_tokenizer = sp.SentencePieceProcessor()\n",
    "        raw_tokenizer.Load(tokenizer_path)\n",
    "\n",
    "        prompt_template = file_processor.get_prompt_template()\n",
    "      else:\n",
    "          raise ValueError(f\"Unsupported file type: {filename}\")\n",
    "    except Exception as e:\n",
    "      raise ValueError(f\"Failed to load model from {filename}: {e}\"\n",
    "          \"Failed to obtain tokenizer from %s: %s\",\n",
    "          tokenizer_location,\n",
    "          e,\n",
    "      )\n",
    "\n",
    "    # Wrap the loaded tokenizer\n",
    "    tokenizer = tokenizer_lib.Tokenizer(raw_tokenizer, prompt_template)\n",
    "\n",
    "    # Load the interpreter\n",
    "    print(\"Loading TFLite model from: %s\", model_path)\n",
    "    try:\n",
    "      interpreter = interpreter_lib.InterpreterWithCustomOps(\n",
    "          custom_op_registerers=[\"pywrap_genai_ops.GenAIOpsRegisterer\"],\n",
    "          model_path=model_path,\n",
    "          num_threads=2,  # Consider making num_threads configurable\n",
    "          experimental_default_delegate_latest_features=True,\n",
    "      )\n",
    "    except Exception as e:\n",
    "      raise ValueError(\n",
    "          \"Failed to load TFLite interpreter from %s: %s\", model_path, e\n",
    "      )\n",
    "      raise\n",
    "\n",
    "    # Create and return the pipeline with the wrapped tokenizer\n",
    "    pipeline = LiteRTLlmPipeline(interpreter, tokenizer)\n",
    "    print(\"LiteRTLlmPipeline loaded successfully.\")\n",
    "    return pipeline"
   ],
   "id": "93aaf1964ef47d64",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T08:19:25.987411Z",
     "start_time": "2025-05-24T08:19:21.661845Z"
    }
   },
   "cell_type": "code",
   "source": "runner = load('gemma3-1b-it-int4.task')",
   "id": "a3ebe4d55ec13af6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TFLite model from: %s cache/TF_LITE_PREFILL_DECODE\n",
      "LiteRTLlmPipeline loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T08:41:49.409561Z",
     "start_time": "2025-05-24T08:41:47.353159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"สวัสดีครับ\"\n",
    "output = runner.generate(prompt, max_decode_steps=None)\n",
    "print(output)"
   ],
   "id": "b7b6fb2ae5b71374",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "สวัสดีครับ! ยินดีค่ะ/ครับ! ถ้าคุณต้องการอะไรจากผม/ฉันบ้าง บอกได้เลยค่ะ/ครับ\n",
      "\n",
      "สวัสดีครับ! ยินดีค่ะ/ครับ! ถ้าคุณต้องการอะไรจากผม/ฉันบ้าง บอกได้เลยค่ะ/ครับ\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19a944eff3c22915"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
